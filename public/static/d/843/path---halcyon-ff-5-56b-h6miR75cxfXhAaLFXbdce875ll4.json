{"data":{"markdownRemark":{"html":"<p><a href=\"https://news.ycombinator.com/item?id=18735903\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://news.ycombinator.com/item?id=18735903</a>\n<a href=\"https://twitter.com/IrisVanRooij/status/1079520922384695296\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://twitter.com/IrisVanRooij/status/1079520922384695296</a></p>\n<h2>What we learned from micro-organisms that solve intractable problems</h2>\n<p>This question is also relevant for issues of intractability in cognitive science. It is conjectured that no NP-hard problem can be tractably computed by quantum computation.</p>\n<p>The amoeba solving the TSP NP-hard problem is a beautiful and shocking example of this principles described in this snippet. </p>\n<p>It is expected that no NP-hard problem is tractably computable by quantum computer. </p>\n<h2>NP-complete Problems and Physical Reality</h2>\n<p><a href=\"https://www.scottaaronson.com/papers/npcomplete.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.scottaaronson.com/papers/npcomplete.pdf</a></p>\n<hr>\n<h2>Shannon</h2>\n<h3>Information Entropy</h3>\n<p>In 1948, Claude Shannon, a young engineer and mathematician working at the Bell Telephone Laboratories, published \"A Mathematical Theory of Communication,\" a seminal paper that marked the birth of information theory. In that paper, Shannon defined what <strong>\"information\"</strong> meant for communication engineers and proposed a precise way to quantify it-in his theory, the fundamental unit of information is the bit.</p>\n<div style=\"background:#D4D3D3; padding: 10px; margin: 40px; box-shadow: 0 6px 8px 0 rgba(0,0,0,0.2)\"> \nHe also showed how data could be \"compressed\" before transmission and how virtually error-free communication could be achieved. The concepts Shannon developed in his paper are at the heart of today's digital information technology. CDs, DVDs, cell phones, fax machines, modems, computer networks, hard drives, memory chips, encryption schemes, MP3 music, optical communication, high-definition television-all these things embody many of Shannon's ideas.\nWhat he showed is you can communicate reliably even though the communication medium is unreliable; that's what digital means.\n</div>\n<p>The one that's most interesting for me is he proved the first threshold theorem. What that means is I could send my voice to you today as a wave, or I could send it to you as a symbol. What he showed is if I send it to you as a symbol, for a linear increase in the resource used to represent the symbol, there is an exponential reduction in the error of you getting the symbol correctly as long as the noise is below a threshold. If the noise is above the threshold, you're doomed. If it's below a threshold, a linear increase in the symbol gives you an exponential reduction in error.</p>\n<hr>\n<p>(see quote tweet for more details).\nAt this  moment I would like to echoe the deep ethos by Social Capital</p>\n<p> there are\na lot of hard problems in the world that computers cannot solve, but that biology can.\nFor many of these problems, the highest-potential path to fixing them lies in the overlap of\ncomputers and biology: computational biology. Computational biology is an emerging discipline\nthat generally refers to two overlapping fields: 1) the practice of taking everything we’ve learned\nabout how to build computers and applying that knowledge to building cells as a programmable,\nflexible, platform with which we can do useful work, and 2) productizing and automating the\ntools, processes, and methods we use in the lab to manipulate biology and build living systems.\nAlthough we’ve gone through a few waves of “biotech bubbles” over the past twenty years, this\ntime may no longer only be about wildly speculative drug development, but instead about\nsomething more concrete and foundational. We'll be able to establish biological systems as\nengineered, all-purpose platforms that we can put to work the same way we do with computers.\nAdditionally, within a few years, we’ll reach a convergence point where our recent advances will\nstart to overlap, and eventually blend, into our existing computing frameworks and\ninfrastructure. This will have a profound and disruptive effect on many fields such as drug design\nand discovery, drug delivery, precision diagnostics and healthcare, engineered materials,\necology, agriculture, and much more. We’ll be able to work with biology in ways that\nincreasingly resemble the way we work with software: as a platform for building tools,\napplications, and infrastructure. This time, however, we’ll be able to do it using living systems\ninstead of code.</p>\n<p>Social Capital </p>\n<p>Digital machines generally beat cells for information processing jobs that demand precision and efficiency. But for jobs that require redundancy, adaptability and resilience in the face of the unknown? Living systems ftw. </p>","frontmatter":{"path":"/halcyon","title":"Biological Informatics","date":"2014-03-23"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/halcyon/"}}